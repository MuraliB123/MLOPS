{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLmEBRQVn76tIuH9og4TMi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuraliB123/MLOPS/blob/master/BERT_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "c4Ns1GpPMMYy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "dHw-qidtqT27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adUqJ_a60BFQ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizerFast\n",
        "\n",
        "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True,)\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "-I3n84eeMwLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "finer_train = datasets.load_dataset(\"nlpaueb/finer-139\", split=\"test\")"
      ],
      "metadata": {
        "id": "gGP-sckgqJ6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finer_train = finer_train.select(range(30))"
      ],
      "metadata": {
        "id": "lNaFHtbkqkJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    # while tokenising sub words are created by tokeniser, our approach is to assign the label of the root word to these sub words.\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        # word_ids is the list consisting of id of original word from which the token has arrived.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(label[word_idx])\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "ffT_khwfkvhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tokenised_dataset = finer_train.map(tokenize_and_align_labels, batched=True)\n"
      ],
      "metadata": {
        "id": "O0myI_Hgk4mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_tokenised_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baO27PK2lFG7",
        "outputId": "ebe6e044-d4b4-41e4-bd1e-b4555e4f5057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 30\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids - integer encoding of the tokens\n",
        "# labels    - corresponding tags"
      ],
      "metadata": {
        "id": "DcRV_iCTngtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections import OrderedDict\n",
        "\n",
        "def bert_text_preparation(tokenized_text,indexed_tokens,tokenizer):\n",
        "    segments_ids = [1] * len(indexed_tokens)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensor = torch.tensor([segments_ids])\n",
        "    return tokenized_text, tokens_tensor, segments_tensor\n",
        "\n",
        "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensor)\n",
        "        hidden_states = outputs[2]\n",
        "\n",
        "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "    # token_embeddings size is [13, 1, x, 768]\n",
        "    # where 13 layers,1 refers batch size, x refers to tokens, 768 refers features in each layer\n",
        "    # remove dimension 1, the \"batches\"\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "    # [13, x, 768]\n",
        "    # swap dimensions 0 and 1 so we can loop over tokens\n",
        "    token_embeddings = token_embeddings.permute(1, 0, 2)\n",
        "    # [x,13,768]\n",
        "    # intialized list to store embeddings\n",
        "    token_vecs_sum = []\n",
        "\n",
        "    # \"token_embeddings\" is a [X x 13 x 768] tensor\n",
        "    # where Y is the number of tokens in the sentence\n",
        "\n",
        "    # loop over tokens in sentence\n",
        "    for token in token_embeddings:\n",
        "\n",
        "        # \"token\" is a [13 x 768] tensor\n",
        "\n",
        "        # sum the vectors from the last four layers\n",
        "        sum_vec = torch.sum(token[-4:], dim=0)\n",
        "        token_vecs_sum.append(sum_vec)\n",
        "\n",
        "    return token_vecs_sum"
      ],
      "metadata": {
        "id": "TtXzVbFVh05O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids    = sample_tokenised_dataset[\"input_ids\"]\n",
        "input_tags   = sample_tokenised_dataset[\"labels\"]\n",
        "\n",
        "context_embeddings = []\n",
        "final_tokens = []\n",
        "token_labels   = []\n",
        "\n",
        "for i in range(0,len(input_ids)):\n",
        "    tags     = input_tags[i]\n",
        "    ids      = input_ids[i]\n",
        "    sentence = tokenizer.batch_decode(ids)\n",
        "\n",
        "    for tag in tags:\n",
        "      if tag!= -100:\n",
        "        token_labels.append(tag)\n",
        "\n",
        "    tokenized_text, tokens_tensor, segments_tensor = bert_text_preparation(sentence,ids, tokenizer)\n",
        "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensor, model)\n",
        "\n",
        "    # make ordered dictionary to keep track of the position of each word\n",
        "    tokens = OrderedDict()\n",
        "    size = len(tokenized_text)\n",
        "    for i in range(1,size-1):\n",
        "        token = tokenized_text[i]\n",
        "        # keep track of position of word and whether it occurs multiple times\n",
        "        if token in tokens:\n",
        "            tokens[token] += 1\n",
        "        else:\n",
        "            tokens[token] = 1\n",
        "        # compute the position of the current token\n",
        "        token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
        "        current_index = token_indices[tokens[token] - 1]\n",
        "        token_vec = list_token_embeddings[current_index]\n",
        "        final_tokens.append(token)\n",
        "        context_embeddings.append(token_vec)\n"
      ],
      "metadata": {
        "id": "AwxzVTWlqoOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(context_embeddings),len(final_tokens),len(token_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q8YVkoFu31q",
        "outputId": "46679fad-c632-41cd-b2f7-fa8130ffac10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1792 1792 1792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hash = {'token' : final_tokens,'embedding' : context_embeddings,'label' : token_labels}\n",
        "dataset = pd.DataFrame(hash)\n",
        "print(dataset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwpqP_QLJn6S",
        "outputId": "39a156f2-0da2-4e57-e395-edad5d95abbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     token                                          embedding  label\n",
            "0      the  [tensor(-2.4772), tensor(-4.8490), tensor(1.76...      0\n",
            "1  changes  [tensor(-2.8018), tensor(1.4222), tensor(2.313...      0\n",
            "2       in  [tensor(-5.0774), tensor(0.6679), tensor(-2.34...      0\n",
            "3      the  [tensor(-1.6575), tensor(-1.2954), tensor(1.04...      0\n",
            "4     fair  [tensor(4.8718), tensor(-0.2470), tensor(3.063...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "Y = dataset[\"label\"]\n",
        "X = dataset[\"embedding\"]\n",
        "X = np.array([np.array(embedding) for embedding in X ])\n",
        "pca = PCA(n_components=10)\n",
        "X = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "_eM77wKaKyBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIjUpNCjOzNU",
        "outputId": "2a01e9ec-9346-403e-f4ba-ecd2e8c28736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 16.3946      -4.8062944    0.7270676  ...  -5.1078415   -1.4797348\n",
            "    3.8995507 ]\n",
            " [ 13.587046     2.594197    10.825192   ...  -0.39473566  -6.480942\n",
            "    8.210312  ]\n",
            " [ 19.653809     1.6561687   -1.7764463  ...   2.5668957   -9.5782\n",
            "   10.238576  ]\n",
            " ...\n",
            " [-40.628166    19.958746   -12.894397   ...  -5.8652463   12.640239\n",
            "    5.40856   ]\n",
            " [ -7.0131874   18.649908   -22.009874   ...  -3.2943673    8.68596\n",
            "    7.8456564 ]\n",
            " [-17.449858   -16.172169    -0.29137462 ...   3.4184458   -2.55471\n",
            "    2.6662385 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)"
      ],
      "metadata": {
        "id": "fYAvfFacNzqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(multi_class='ovr') # one vs rest for multi class classification\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHbONfdEN_kq",
        "outputId": "2deb71ab-3d5c-449f-f0e9-bbf11e83485d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9916434540389972\n"
          ]
        }
      ]
    }
  ]
}